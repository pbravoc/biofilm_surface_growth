{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from glob import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def datx2py(file_name):\n",
    "    \"\"\"Loads a .datx into Python, credit goes to gkaplan.\n",
    "    https://gist.github.com/g-s-k/ccffb1e84df065a690e554f4b40cfd3a\"\"\"\n",
    "    def _group2dict(obj):\n",
    "        return {k: _decode_h5(v) for k, v in zip(obj.keys(), obj.values())}\n",
    "    def _struct2dict(obj):\n",
    "        names = obj.dtype.names\n",
    "        return [dict(zip(names, _decode_h5(record))) for record in obj]\n",
    "    def _decode_h5(obj):\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            d = _group2dict(obj)\n",
    "            if len(obj.attrs):\n",
    "                d['attrs'] = _decode_h5(obj.attrs)\n",
    "            return d\n",
    "        elif isinstance(obj, h5py.AttributeManager):\n",
    "            return _group2dict(obj)\n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            d = {'attrs': _decode_h5(obj.attrs)}\n",
    "            try:\n",
    "                d['vals'] = obj[()]\n",
    "            except (OSError, TypeError):\n",
    "                pass\n",
    "            return d\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            if np.issubdtype(obj.dtype, np.number) and obj.shape == (1,):\n",
    "                return obj[0]\n",
    "            elif obj.dtype == 'object':\n",
    "                return _decode_h5([_decode_h5(o) for o in obj])\n",
    "            elif np.issubdtype(obj.dtype, np.void):\n",
    "                return _decode_h5(_struct2dict(obj))\n",
    "            else:\n",
    "                return obj\n",
    "        elif isinstance(obj, np.void):\n",
    "            return _decode_h5([_decode_h5(o) for o in obj])\n",
    "        elif isinstance(obj, bytes):\n",
    "            return obj.decode()\n",
    "        elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "            if len(obj) == 1:\n",
    "                return obj[0]\n",
    "            else:\n",
    "                return obj\n",
    "        else:\n",
    "            return obj\n",
    "    with h5py.File(file_name, 'r') as f:\n",
    "        h5data = _decode_h5(f)\n",
    "    return h5data\n",
    "\n",
    "def get_data(datx_file):\n",
    "    \"\"\"Returns the Surface and Intensity data from a single .datx file\"\"\"\n",
    "    myh5 = datx2py(datx_file)                      # File is the string with the location of the file\n",
    "    zsurf = myh5['Data']['Surface']           # Get the surfaces\n",
    "    zdata = list(zsurf.values())[0]           # Good for fixing stuff later  \n",
    "    zsurf = zdata['vals']                     # Get the data from the surface group\n",
    "    zsurf[zsurf == zdata['attrs']['No Data']] = np.nan  # Write no data as NaNs for compatibility\n",
    "    zint = myh5['Data']['Intensity']          # Get the intensity group\n",
    "    zint = list(zint.values())[0]['vals'].astype(float)  # Get the data from the intensity grou[]\n",
    "    zint[zint>200000] = np.nan                # This fixes the regions left out from stitching\n",
    "    return zsurf, zint\n",
    "    \n",
    "def getcleansurf(img):\n",
    "    suma = np.nansum(img, axis=0)\n",
    "    counts = np.sum(~np.isnan(img), axis=0)\n",
    "    counts[counts<20] = 0\n",
    "    f = suma/counts\n",
    "    f[np.isinf(f)] = np.nan\n",
    "    return f\n",
    "\n",
    "def find_edge(y, v):\n",
    "    g2 = np.abs(np.gradient(y))\n",
    "    idx = np.argwhere(g2>v)\n",
    "    l = idx[0][0]\n",
    "    h = idx[-1][0]\n",
    "    return l,h\n",
    "\n",
    "def subline(f, offx, offy):\n",
    "    xchange = -len(f)\n",
    "    ychange = (f[0]-offx)-(f[-1]-offy)\n",
    "    xarray = np.arange(len(f))\n",
    "    yarray = xarray*(ychange/xchange) + (f[0]-offx)\n",
    "    res = (f-yarray)\n",
    "    return res\n",
    "\n",
    "def plot_bounds(i, C, l):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(first_pass[i])\n",
    "    plt.axvline(C[i,0], color=\"black\", linestyle=\"dashed\")\n",
    "    plt.axvline(C[i,1], color=\"black\", linestyle=\"dashed\")\n",
    "    plt.subplot(223)\n",
    "    plt.plot(first_pass[i])\n",
    "    plt.axvline(C[i,0], color=\"black\", linestyle=\"dashed\")\n",
    "    plt.axvline(C[i,1], color=\"black\", linestyle=\"dashed\")\n",
    "    plt.xlim(C[i,0]-l, C[i,0]+l)\n",
    "    plt.subplot(224)\n",
    "    plt.plot(first_pass[i])\n",
    "    plt.axvline(C[i,0], color=\"black\", linestyle=\"dashed\")\n",
    "    plt.axvline(C[i,1], color=\"black\", linestyle=\"dashed\")\n",
    "    plt.xlim(C[i,1]-l, C[i,1]+l)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plotrel_profile(i, C, o):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    s_past = subline(first_pass[i-1][C[i-1,0]:C[i-1,1]]/1e3, o[i-1,0], o[i-1,1])\n",
    "    s_curr = subline(first_pass[i][C[i,0]:C[i,1]]/1e3, o[i,0], o[i,1])\n",
    "    s_futu = subline(first_pass[i+1][C[i+1,0]:C[i+1,1]]/1e3, o[i+1,0], o[i+1,1])\n",
    "\n",
    "    plt.plot(s_past, label=\"Past\")\n",
    "    plt.plot(s_curr, label=\"Current\")\n",
    "    plt.plot(s_futu, label=\"Future\")\n",
    "    plt.axhline(np.nanmax(s_curr), color=\"black\", linestyle=\"--\")\n",
    "    plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "\n",
    "def shift(n_s, f):\n",
    "    start[n_s:] += f*np.arange(n-n_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean array from multiple .datx\n",
    "\n",
    "This is slow and takes some user corrections, but it has to get done. \n",
    "\n",
    "The Interferometer tends to fail when the slope is *locally* too large, if we pair that with our FOV being 1000x200, full vertical lines of `NaN` can be common (specially in the edges of the biofilm!). This leads the stitching process (from the ZYGO software) to act weird sometimes, and producing errors. \n",
    "\n",
    "In order to generate a clean dataset, we use a two step process:\n",
    "1. Find the biofilm-LB edges on both sides (if edge is out of the FOV is not all lost!)\n",
    "2. Substract a plane from edge-to-edge. Discuss: earlier times may require a poly2.\n",
    "3. Correct profiles that aren't consistent. By looking at *previous* and *future* timepoints, we have boundaries for where the *current* one should be. This involves some tweaking of the offsets on LHS and RHS.\n",
    "\n",
    "We are allowed to do this for two reasons:\n",
    "1. Temporal resolution is *very good* there is very little growth between $t$ and $t+1$.\n",
    "2. Analysis on surface fluctuations is **not affected** by these corrections, since we use only the homeland. We have tested also multiple substraction algorithms and they aren't much different from the raw profiles.\n",
    "\n",
    "So if it doesn't matter on the surface analysis, why should we do it? Because it makes data clearer and clean! It allows for all the other analysis down the pipeline be able to run as short scripts. Clean data is also easier to share, and it makes **pretty plots**.\n",
    "\n",
    "Simple fixes that do not require justification are the numbered sections 1-2-3. More information on each of these procedures is given on its corresponding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load TIMELAPSE files and get times\n",
    "\n",
    "#folder_tag = \"2021-08-27_jt305\"\n",
    "#folder_tag = \"2021-09-03_pyeast\"\n",
    "#folder_tag = \"2021-06-25_bgt127\"\n",
    "#folder_tag = \"2021-07-30_bacillus\"\n",
    "#folder_tag = \"2021-10-28_petiteyeastlong\"\n",
    "#folder_tag = \"2022-01-28_aerobicyeast\"\n",
    "folder_tag = \"2022-02-04_pa01\"\n",
    "\n",
    "folder = \"/run/media/pablo/T7/Documents/Research/Biofilms/Data/Interferometry/radial_timelapses/\"+folder_tag+\"/\"\n",
    "files = glob(folder+\"Raw/*.datx\")                                           # Folder with all the .datx\n",
    "files.sort()\n",
    "n_replicates = 3                                                            # Number of timelapse replicates (numbered)\n",
    "n_controls = 3\n",
    "n = int(len(glob(folder+\"Raw/*[0-9][0-9][0-9].datx\"))/n_replicates)         # Timepoints for each replicate\n",
    "files_control = files[-n_controls:]                                                 # These are ALL the un-numbered files\n",
    "idx = np.arange(n)*n_replicates    # Timepoints x replicates                # Total timelapse measurements\n",
    "files_split = [[] for i in range(n_replicates)]                             \n",
    "for i in idx:\n",
    "    for j in range(n_replicates):\n",
    "        files_split[j].append(files[i+j])\n",
    "\n",
    "tstart = 1644000200.0\n",
    "times_tl = np.zeros([n, n_replicates])                                          # Array of all timelapse relative time measurements (hours)\n",
    "times_cl = np.zeros(len(files_control))                                # Array of all control relative time measurements (hours)\n",
    "for i in range(n_replicates):\n",
    "    for j in range(n):\n",
    "        times_tl[j,i] = (os.path.getmtime(files_split[i][j])-tstart)/3600\n",
    "for i in range(len(files_control)):\n",
    "    times_cl[i] = (os.path.getmtime(files_control[i])-tstart)/3600\n",
    "\n",
    "offnames = [folder+\"Clean/offsets_A.npy\", folder+\"Clean/offsets_B.npy\", folder+\"Clean/offsets_C.npy\"]\n",
    "Cnames = [folder+\"Clean/bounds_A.npy\", folder+\"Clean/bounds_B.npy\", folder+\"Clean/bounds_C.npy\"]\n",
    "dnames = [folder+\"Clean/displacement_A.npy\", folder+\"Clean/displacement_B.npy\", folder+\"Clean/displacement_C.npy\"]\n",
    "\n",
    "C = np.zeros([n, 2]).astype(int)\n",
    "offsets = np.zeros([n, 2]).astype(float)\n",
    "print(times_tl[0,0])\n",
    "np.save(folder+\"Clean/times.npy\", times_tl)\n",
    "np.save(folder+\"Clean/times_control.npy\", times_cl)\n",
    "\n",
    "# If running all the controls\n",
    "#C = np.zeros([len(files_control), 2]).astype(int)\n",
    "#offsets = np.zeros([len(files_control), 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Biofilm edges\n",
    "\n",
    "In this step we find the left `l` and right `r` edges in the biofilm profile, we use the function `find_edge(profile)`, which will automatically find them for us. Since experimental data is subject to the Zygo stitching algorithm, the algorithm *may* fail, adjusting the edge location **should** be done before the other steps. \n",
    "`l` and `r` and then stored in an Array containing all the edge locations `C`.\n",
    "For this we first lower the dimensionality of all the timelapse `j` into a list of profiles called `first_pass`. That way when we call them individually and plot against each other they are ready to plot.\n",
    "\n",
    "Then, we loop over the indivdual profiles `i` while also plotting the previous `i-1` and future `i+1` timesteps. We start from a base offset of 0, that should output the relative height of the profiles in relation to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the timelapse here with j\n",
    "j = 0                                           # Which of the replicates we'll be working on\n",
    "first_pass = []                                 # We'll do a naive detection of the edges\n",
    "for i in range(n):\n",
    "    s = getcleansurf(get_data(files_split[j][i])[0])# Load profiles as a 1-D array\n",
    "    #s = getcleansurf(get_data(files_control[i])[0]) # Load control profiles as a 1-D array\n",
    "    first_pass.append(s)\n",
    "    lim_edge = 10\n",
    "    if i>30:\n",
    "        lim_edge = 20\n",
    "    if i>90:\n",
    "        lim_edge=40\n",
    "    l, r = find_edge(s,lim_edge)                      # Here, 15 is the detection threshold\n",
    "    C[i] = np.array([l,r])\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C[30:50,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we loop over each profile\n",
    "i = 83\n",
    "#C[i,0] = C[i-1,0]-100\n",
    "C[i,1] = C[i-1,1]+00\n",
    "\n",
    "plot_bounds(i, C, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(folder+\"Clean/bounds_A.npy\", C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leveling the profiles\n",
    "\n",
    "This is the slowest part, since it involves looking at 3 consecutive profiles, and correcting for height displacements. For *most* of the timelapse, you should aim to have the <span style=\"color:#FF7F0E\">**current**</span> profile between the <span style=\"color:#1F77B4\">**past** </span> and <span style=\"color:#2CA02C\">**future**</span> ones.\n",
    "While some particular timepoints are off just because of small miscalculations in finding the edges, more prevalent issues may arise when the actual biofilm-lb border goes out of focus in either the in or out of plane direction. This will lead to a few consecutive offsets in the same side of the sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "i = 47\n",
    "#offsets[143,1] = \n",
    "offsets[i,0] += 0.0\n",
    "offsets[i,1] += 0.0\n",
    "plotrel_profile(i, C, offsets)\n",
    "#plt.ylim(0, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(folder+\"Clean/offsets_A.npy\", offsets)\n",
    "np.save(folder+\"Clean/bounds_A.npy\", C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Displacement alignment\n",
    "\n",
    "Now we have a set of profile heights that correspond to the same timelapse. Since we work using the center of the colonies, it would be useful to have everything aligned, so we can get the data quickly.\n",
    "\n",
    "Of course, the size of the first inoculum is much less than the final point. For this, we center everything into a 2D array with ALL the timelapse profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For timelapses\n",
    "j = 0                                          # Which of the replicates we'll be working on\n",
    "offsets = np.load(offnames[j])\n",
    "C = np.load(Cnames[j]).astype(int)\n",
    "data = []                                 # We'll do a naive detection of the edges\n",
    "for i in range(n):\n",
    "    s = getcleansurf(get_data(files_split[j][i])[0])# Load profiles as a 1-D array\n",
    "    s_curr = subline(s[C[i,0]:C[i,1]]/1e3, offsets[i,0], offsets[i,1]) # Use edges and offsets to subtract\n",
    "    data.append(s_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For controls\n",
    "#offsets = np.load(folder+\"/Clean/offsets_controls.npy\")\n",
    "C = np.load(folder+\"/Clean/bounds_control.npy\").astype(int)\n",
    "data = []                                 # We'll do a naive detection of the edges\n",
    "for i in range(n_controls):\n",
    "    s = getcleansurf(get_data(files_control[i])[0])# Load profiles as a 1-D array\n",
    "    s_curr = subline(s[C[i,0]:C[i,1]]/1e3, offsets[i,0], offsets[i,1]) # Use edges and offsets to subtract\n",
    "    data.append(s_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.array([len(x) for x in data])\n",
    "np.max(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 34000\n",
    "IMG = np.zeros([len(data), L])\n",
    "IMG[:] = np.nan\n",
    "\n",
    "start = (L-S)/2\n",
    "start = start.astype(int)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    IMG[i, start[i]:len(data[i])+start[i]] = data[i]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(IMG, aspect=50, clim=(0, 200), origin=\"lower\", cmap=\"turbo\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.transpose(IMG));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(folder+\"Clean/bounds_control.npy\", C)\n",
    "np.save(folder+\"Clean/offsets_control.npy\", offsets)\n",
    "np.save(folder+\"Clean/profiles_control.npy\", IMG)\n",
    "np.save(folder+\"Clean/displacement_control.npy\", start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save everything into a .csv\n",
    "\n",
    "Now we have all the required building blocks for our .csv, for each of [A, B, C, control]:\n",
    "1. Profiles\n",
    "2. Boundaries\n",
    "3. Offset\n",
    "4. Displacement\n",
    "\n",
    "Now we load everything into a simple panda dataframe, and export it as .csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'file':[], 'replicate':[], 'time':[], 'border_l':[], 'border_r':[], 'offset_l':[], 'offest_r':[], 'displacement':[]})\n",
    "\n",
    "replicate = [\"A\", \"B\", \"C\"]\n",
    "times = np.load(folder+\"Clean/times.npy\")\n",
    "# Loop over ABC\n",
    "for j in range(3):\n",
    "    offsets = np.load(offnames[j])\n",
    "    C = np.load(Cnames[j])\n",
    "    displ = np.load(dnames[j])\n",
    "    t = times[:,j]\n",
    "    repl = np.repeat(replicate[j], len(t))\n",
    "    fnames = [name[len(folder)+4:] for name in files_split[j]]\n",
    "    b_l, b_r = C[:,0], C[:,1]\n",
    "    o_l, o_r = offsets[:,0], offsets[:,1]\n",
    "    tf = pd.DataFrame({'file':fnames, 'replicate':repl, 'time':t, 'border_l':b_l, 'border_r':b_r, 'offset_l':o_l, 'offest_r':o_r, 'displacement':displ})\n",
    "    df = df.append(tf)\n",
    "offsets = np.load(folder+\"Clean/offsets_control.npy\")\n",
    "C = np.load(folder+\"Clean/bounds_control.npy\")\n",
    "displ = np.load(folder+\"Clean/displacement_control.npy\")\n",
    "t = np.load(folder+\"Clean/times_control.npy\")\n",
    "repl = np.repeat(replicate[j], len(t))\n",
    "fnames = [name[len(folder)+4:] for name in files_control]\n",
    "b_l, b_r = C[:,0], C[:,1]\n",
    "o_l, o_r = offsets[:,0], offsets[:,1]\n",
    "tf = pd.DataFrame({'file':fnames, 'replicate':repl, 'time':t, 'border_l':b_l, 'border_r':b_r, 'offset_l':o_l, 'offest_r':o_r, 'displacement':displ})\n",
    "df = df.append(tf)\n",
    "df.to_csv(folder+\"Clean/cleaning.csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
